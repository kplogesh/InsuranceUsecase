apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-configmap
  namespace: default
data:
  logstash.yml: |
    http.host: "0.0.0.0"
    path.config: /usr/share/logstash/pipeline
    xpack.monitoring.enabled: false
  logstash.conf: |
    # all input will come from filebeat, no local logs
    input 
    {
      beats {
        port => 5044
      }
    }

    filter 
    {
      if [fields][filename] == "trxAudit.log" 
      {      
        clone 
        {
            clones => ["cloned"]
        }       
        json
        {
            source => "message"
            target => "auditdata"
        }     
        mutate 
        {
            remove_field => [ "message", "log", "host", "agent", "input", "ecs","tags", "@version","@timestamp"]
            add_field => 
            { 
                "status" => "%{[auditdata][auditType]}" 
                "creationDate" => "%{[auditdata][timestamp]}"
                "nativeURL" => "%{[auditdata][loggerData][loggerName]}"
                "logMessage" => "%{[auditdata][loggerData][logMessage]}"
                "server" => "%{[auditdata][serverName]}:%{[auditdata][serverPort]}" 
            }
        }
        if [status] == "START"
        {
            mutate
            {
                add_field => {"reqPayload" => "%{[auditdata][docToBeLogged]}"}
            }
        }
        else
        {
            mutate
            {
                add_field => {"resPayload" => "%{[auditdata][docToBeLogged]}"}
            }
        }
        # Ruby section below is for node graph. If node visualization is not required, then below section can be commented!
        # Custom header section will be dynamically mapped and assigned as key value pairs
        ruby
        {
            code => '
              event.get("[auditdata][customHeaders]").each { |k, v|
                  event.set(k, v)
              }

              if event.get("x-transaction-id").nil?; event.set("x-transaction-id", rand(36**10).to_s(36)); end

              parent=event.get("nativeURL").split("|||").first;
              child=event.get("nativeURL").split("|||").last;

              if parent.include?("[SOURCE]") then
                event.set("source",event.get("x-transaction-id")+"_"+child);
                event.set("subtitle",parent); 
              else
                event.set("source",event.get("x-transaction-id")+"_"+parent);
              end

              event.set("target",event.get("x-transaction-id")+"_"+child);
              
              event.set("id",event.get("x-transaction-id")+"_"+child);
              event.set("mainstat",child.split(".").last);
              event.set("title",child);
              
              unless event.get("status").include?("START")
                event.set("arc__"+event.get("status").downcase,1);
              '
        }
        if [type] == "cloned" 
        {
            prune 
            {
                blacklist_names => ["auditdata", "title", "nativeURL", "status", "resPayload", "reqPayload", "serverName","mainstat","logMessage", "subtitle","arc__error","arc__complete","arc__start"]
            }
        }       
        else
        {
            prune 
            {
                blacklist_names => ["auditdata", "source", "target"]
            }          
        }  
        date 
        {
            match => [ "creationDate", "YYYY-MM-dd HH:mm:ss.SSS" ]
            target => "creationDate"
        }
      }     
    }
   
    output 
    {
        if [fields][filename] == "trxAudit.log"
        {
           if [type] == "cloned" 
           {
                elasticsearch  
                {              
                    hosts => [ "${ELASTICSEARCH_HOSTS}" ]
                    user => "${ELASTICSEARCH_USER}"
                    password => "${ELASTICSEARCH_PASSWORD}"
                    index => "edgefields"
                    doc_as_upsert => "true"
                    action => "update"
                    document_id => "%{[source]}_%{[target]}"
                }
           }
           else
           {   
                elasticsearch  
                {              
                    hosts => [ "${ELASTICSEARCH_HOSTS}" ]
                    user => "${ELASTICSEARCH_USER}"
                    password => "${ELASTICSEARCH_PASSWORD}"
                    index => "customlogger_transactionalevents"
                    doc_as_upsert => "true"
                    action => "update"
                    document_id => "%{[x-transaction-id]}_%{[nativeURL]}"
                }
           }
                      stdout { codec => rubydebug }
        } 
        else 
        {
                elasticsearch  
                {
                      hosts => [ "${ELASTICSEARCH_HOSTS}" ]
                      user => "${ELASTICSEARCH_USER}"
                      password => "${ELASTICSEARCH_PASSWORD}"
                      #index => "logstash-noapplabel_%{[@metadata][beat]}-%{+YYYY.MM.dd}"
                      index => "logstash-noapplabel-%{[@metadata][beat]}"
                }
        }          
    }
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: logstash
  name: logstash
spec:
  containers:
  - image: docker.elastic.co/logstash/logstash:7.13.0
    name: logstash
    ports:
    - containerPort: 5044
    env:
    - name: ELASTICSEARCH_HOSTS
      value: "http://api-gateway-service:9240"
    - name: ELASTICSEARCH_USER
      value: "elastic"
    - name: ELASTICSEARCH_PASSWORD
      value: "changeme"
    resources: {}
    volumeMounts:
    - name: config-volume
      mountPath: /usr/share/logstash/config
    - name: logstash-pipeline-volume
      mountPath: /usr/share/logstash/pipeline
  restartPolicy: OnFailure
  volumes:
  - name: config-volume
    configMap:
      name: logstash-configmap
      items:
        - key: logstash.yml
          path: logstash.yml
  - name: logstash-pipeline-volume
    configMap:
      name: logstash-configmap
      items:
        - key: logstash.conf
          path: logstash.conf
---
kind: Service
apiVersion: v1
metadata:
  name: logstash-service
  namespace: default
spec:
  selector:
    app: logstash
  ports:
  - protocol: TCP
    port: 5044
    targetPort: 5044
  type: ClusterIP